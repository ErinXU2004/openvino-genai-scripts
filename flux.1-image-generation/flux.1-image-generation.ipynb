{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c31fcb69-002f-47bc-92b5-71b36eb2977e",
   "metadata": {},
   "source": [
    "# Image generation with Flux.1 and OpenVINO\n",
    "\n",
    "Flux is a AI image generation model developed by [Black Forest Labs](https://blackforestlabs.ai/our-team/). It represents a significant advancement in AI-generated art, utilizing a hybrid architecture of [multimodal](https://arxiv.org/abs/2403.03206) and [parallel](https://arxiv.org/abs/2302.05442) [diffusion transformer](https://arxiv.org/abs/2212.09748) blocks and scaled to 12B parameter. The model offers state-of-the-art performance image generation with top of the line prompt following, visual quality, image detail and output diversity. More details about model can be found in [blog post](https://blackforestlabs.ai/announcing-black-forest-labs/) and [original repo](https://github.com/black-forest-labs/flux).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/black-forest-labs/flux/main/assets/grid.jpg\" width=\"1024\" height=\"800\"> \n",
    "\n",
    "In this tutorial we consider how to convert and optimize Flux.1 model using OpenVINO.\n",
    "\n",
    ">**Note**: Some demonstrated models can require at least 32GB RAM for conversion and running.\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Select model](#Select-model)\n",
    "- [Convert model with OpenVINO](#Convert-model-with-OpenVINO)\n",
    "  - [Convert model using Optimum Intel](#Convert-model-using-Optimum-Intel)\n",
    "  - [Compress model weights](#Compress-model-weights)\n",
    "  - [Use optimized models provided on HuggingFace Hub](#use-optimized-models-provided-on-huggingface-hub)\n",
    "- [Run OpenVINO model inference](#Run-OpenVINO-model-inference)\n",
    "- [Interactive demo](#Interactive-demo)\n",
    "\n",
    "\n",
    "\n",
    "<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=5b5a4db0-7875-4bfb-bdbd-01698b5b1a77&file=notebooks/flux.1-image-generation/flux.1-image-generation.ipynb\" />\n",
    "\n",
    "### Installation Instructions\n",
    "\n",
    "This is a self-contained example that relies solely on its own code.\n",
    "\n",
    "We recommend  running the notebook in a virtual environment. You only need a Jupyter server to start.\n",
    "For details, please refer to [Installation Guide](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/README.md#-installation-guide)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97ce9e70-fdce-4187-a0ea-c69e0001e25a",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b05ef521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[5 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "  \u001b[31m   \u001b[0m Collecting setuptools>=40.8.0\n",
      "  \u001b[31m   \u001b[0m \u001b[31m  ERROR: HTTP error 403 while getting https://pypi.tuna.tsinghua.edu.cn/packages/a3/dc/17031897dae0efacfea57dfd3a82fdd2a2aeb58e0ff71b77b87e44edc772/setuptools-80.9.0-py3-none-any.whl (from https://pypi.tuna.tsinghua.edu.cn/simple/setuptools/) (requires-python:>=3.9)\u001b[0m\u001b[31m\n",
      "  \u001b[31m   \u001b[0m \u001b[0m\u001b[31mERROR: Could not install requirement setuptools>=40.8.0 from https://pypi.tuna.tsinghua.edu.cn/packages/a3/dc/17031897dae0efacfea57dfd3a82fdd2a2aeb58e0ff71b77b87e44edc772/setuptools-80.9.0-py3-none-any.whl because of HTTP error 403 Client Error: Forbidden for url: https://pypi.tuna.tsinghua.edu.cn/packages/a3/dc/17031897dae0efacfea57dfd3a82fdd2a2aeb58e0ff71b77b87e44edc772/setuptools-80.9.0-py3-none-any.whl for URL https://pypi.tuna.tsinghua.edu.cn/packages/a3/dc/17031897dae0efacfea57dfd3a82fdd2a2aeb58e0ff71b77b87e44edc772/setuptools-80.9.0-py3-none-any.whl (from https://pypi.tuna.tsinghua.edu.cn/simple/setuptools/) (requires-python:>=3.9)\u001b[0m\u001b[31m\n",
      "  \u001b[31m   \u001b[0m \u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "%pip install -q --upgrade pip\n",
    "\n",
    "%pip install -q torch==2.1.2 torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q \"gradio>=4.19\" \"torch>=2.1\" \"transformers\" \"nncf>=2.15.0\" \"diffusers>=0.31.0\" \"opencv-python\" \"pillow\" \"peft>=0.7.0\" --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q \"sentencepiece\" \"protobuf\"\n",
    "%pip install -q \"git+https://github.com/huggingface/optimum-intel.git\"\n",
    "%pip install -qU \"openvino>=2025.0\" \"openvino_genai>=2025.0\" \"openvino_tokenizers>=2025.0\"\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    %pip install -q \"numpy<2.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64fff924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path(\"cmd_helper.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/cmd_helper.py\")\n",
    "    open(\"cmd_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "if not Path(\"gradio_helper.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/flux.1-image-generation/gradio_helper.py\")\n",
    "    open(\"gradio_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "if not Path(\"notebook_utils.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\")\n",
    "    open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "\n",
    "# Read more about telemetry collection at https://github.com/openvinotoolkit/openvino_notebooks?tab=readme-ov-file#-telemetry\n",
    "from notebook_utils import collect_telemetry\n",
    "\n",
    "collect_telemetry(\"flux.1-image-generation.ipynb\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "592f926e-e4e4-4232-9c20-a986ddfcc4a5",
   "metadata": {},
   "source": [
    "## Select model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "To strike a balance between accessibility and model capabilities, FLUX.1 comes in three variants: FLUX.1-pro, FLUX.1-dev and FLUX.1-schnell: \n",
    "* **FLUX.1-pro**: The best of FLUX.1, offering state-of-the-art performance image generation with top of the line prompt following, visual quality, image detail and output diversity, but not available for public usage.\n",
    "* **FLUX.1-dev**: FLUX.1-dev is an open-weight, guidance-distilled models. Directly distilled from FLUX.1-pro, FLUX.1-dev obtains similar quality and prompt adherence capabilities, while being more efficient than a standard model of the same size. FLUX.1-dev weights are available on [HuggingFace](https://huggingface.co/black-forest-labs/FLUX.1-dev).\n",
    "* **FLUX.1-schnell**: the fastest model from Flux family is tailored for local development and personal use. FLUX.1-schnell is openly available under an Apache2.0 license. Similar, FLUX.1-dev, weights are available on [HuggingFace](https://huggingface.co/black-forest-labs/FLUX.1-schnell).\n",
    "\n",
    "![family.png](https://github.com/user-attachments/assets/c7f9df6b-cff3-4d33-98d7-1bb400b2861c)\n",
    "\n",
    "Be default, we will use FLUX.1-schnell model, but you can switch to FLUX.1-dev version using widget bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b76031c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: ipywidgets in /Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (8.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from ipywidgets) (8.37.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: decorator in /Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in /Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.3.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: wcwidth in /Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd654a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75be6a7ddb574899b3a29198626b1247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', options=('black-forest-labs/FLUX.1-schnell', 'black-forest-labs/FLUX.1-dev'), v…"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "model_ids = [\"black-forest-labs/FLUX.1-schnell\", \"black-forest-labs/FLUX.1-dev\"]\n",
    "\n",
    "model_selector = widgets.Dropdown(\n",
    "    options=model_ids,\n",
    "    default=model_ids[0],\n",
    "    description=\"Model:\",\n",
    ")\n",
    "\n",
    "\n",
    "model_selector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2578fc91-dda4-4a0b-9353-5654d2763905",
   "metadata": {},
   "source": [
    ">**Note**: run Flux.1-dev model with notebook, you will need to accept license agreement. \n",
    ">You must be a registered user in 🤗 Hugging Face Hub. Please visit [HuggingFace model card](https://huggingface.co/black-forest-labs/FLUX.1-dev), carefully read terms of usage and click accept button.  You will need to use an access token for the code below to run. For more information on access tokens, refer to [this section of the documentation](https://huggingface.co/docs/hub/security-tokens).\n",
    ">You can login on Hugging Face Hub in notebook environment, using following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2676359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment these lines to login to huggingfacehub to get access to pretrained model\n",
    "\n",
    "# from huggingface_hub import notebook_login, whoami\n",
    "\n",
    "# try:\n",
    "#     whoami()\n",
    "#     print('Authorization token already provided')\n",
    "# except OSError:\n",
    "#     notebook_login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10fee712-3dbe-4c2e-9bb2-b8a760a4d1f3",
   "metadata": {},
   "source": [
    "## Convert model with OpenVINO\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Starting from 2023.0 release, OpenVINO supports PyTorch models directly via Model Conversion API. `ov.convert_model` function accepts instance of PyTorch model and example inputs for tracing and returns object of `ov.Model` class, ready to use or save on disk using `ov.save_model` function. \n",
    "\n",
    "\n",
    "The pipeline consists of four important parts:\n",
    "\n",
    "* Clip and T5 Text Encoders to create condition to generate an image from a text prompt.\n",
    "* Transformer for step-by-step denoising latent image representation.\n",
    "* Autoencoder (VAE) for decoding latent space to image.\n",
    "  \n",
    "### Convert model using Optimum Intel\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "For convenience, we will use OpenVINO integration with HuggingFace Optimum. 🤗 [Optimum Intel](https://huggingface.co/docs/optimum/intel/index) is the interface between the 🤗 Transformers and Diffusers libraries and the different tools and libraries provided by Intel to accelerate end-to-end pipelines on Intel architectures.\n",
    "\n",
    "Among other use cases, Optimum Intel provides a simple interface to optimize your Transformers and Diffusers models, convert them to the OpenVINO Intermediate Representation (IR) format and run inference using OpenVINO Runtime. `optimum-cli` provides command line interface for model conversion and optimization. \n",
    "\n",
    "General command format:\n",
    "\n",
    "```bash\n",
    "optimum-cli export openvino --model <model_id_or_path> --task <task> <output_dir>\n",
    "```\n",
    "\n",
    "where task is task to export the model for, if not specified, the task will be auto-inferred based on the model. You can find a mapping between tasks and model classes in Optimum TaskManager [documentation](https://huggingface.co/docs/optimum/exporters/task_manager). Additionally, you can specify weights compression using `--weight-format` argument with one of following options: `fp32`, `fp16`, `int8` and `int4`. Fro int8 and int4 [nncf](https://github.com/openvinotoolkit/nncf) will be used for  weight compression. More details about model export provided in [Optimum Intel documentation](https://huggingface.co/docs/optimum/intel/openvino/export#export-your-model).\n",
    "\n",
    "### Compress model weights\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "For reducing model memory consumption we will use weights compression. The [Weights Compression](https://docs.openvino.ai/2024/openvino-workflow/model-optimization-guide/weight-compression.html) algorithm is aimed at compressing the weights of the models and can be used to optimize the model footprint and performance of large models where the size of weights is relatively larger than the size of activations, for example, Large Language Models (LLM). Compared to INT8 compression, INT4 compression improves performance even more, but introduces a minor drop in prediction quality. We will use [NNCF](https://github.com/openvinotoolkit/nncf) integration to `optimum-cli` tool for weight compression.\n",
    "\n",
    "### Use optimized models provided on HuggingFace Hub\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "For quick start, OpenVINO provides [collection](https://huggingface.co/collections/OpenVINO/image-generation-67697d9952fb1eee4a252aa8) of optimized models that are ready to use with OpenVINO GenAI. You can download them using following command:\n",
    "\n",
    "```bash\n",
    "huggingface-cli download <model_id> --local-dir <output_dir>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "add19dd3",
   "metadata": {
    "test_replace": {
     "use_preconverted = widgets.Checkbox(value=\"schnell\" in model_selector.value": "use_preconverted = widgets.Checkbox(value=False"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1a3ff0d08549abbc9344ad6f0a8296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Checkbox(value=True, description='Weight compression'), Checkbox(value=True, description='Use p…"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_preconverted = widgets.Checkbox(value=\"schnell\" in model_selector.value, description=\"Use preconverted model\", disable=False)\n",
    "\n",
    "to_compress = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description=\"Weight compression\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "visible_widgets = [to_compress]\n",
    "\n",
    "if \"schnell\" in model_selector.value:\n",
    "    visible_widgets.append(use_preconverted)\n",
    "\n",
    "options = widgets.VBox(visible_widgets)\n",
    "\n",
    "options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e46869c",
   "metadata": {
    "test_replace": {
     "64": "-1",
     "model_selector.value": "\"katuni4ka/tiny-random-flux\""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_id = model_selector.value\n",
    "\n",
    "model_base_dir = Path(model_id.split(\"/\")[-1])\n",
    "additional_args = {}\n",
    "\n",
    "if to_compress.value:\n",
    "    model_dir = model_base_dir / \"INT4\"\n",
    "    additional_args.update({\"weight-format\": \"int4\", \"group-size\": \"64\", \"ratio\": \"1.0\"})\n",
    "else:\n",
    "    model_dir = model_base_dir / \"FP16\"\n",
    "    additional_args.update({\"weight-format\": \"fp16\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6c7bb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 700, in urlopen\n",
      "    self._prepare_proxy(conn)\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 996, in _prepare_proxy\n",
      "    conn.connect()\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/urllib3/connection.py\", line 414, in connect\n",
      "    self.sock = ssl_wrap_socket(\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/ssl.py\", line 513, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/ssl.py\", line 1104, in _create\n",
      "    self.do_handshake()\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/ssl.py\", line 1375, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLEOFError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1017)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/requests/adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/urllib3/util/retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/OpenVINO/FLUX.1-schnell-int4-ov/revision/main (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1017)')))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/bin/huggingface-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 59, in main\n",
      "    service.run()\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/huggingface_hub/commands/download.py\", line 153, in run\n",
      "    print(self._download())  # Print path to downloaded files\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/huggingface_hub/commands/download.py\", line 187, in _download\n",
      "    return snapshot_download(\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/huggingface_hub/_snapshot_download.py\", line 165, in snapshot_download\n",
      "    repo_info = api.repo_info(repo_id=repo_id, repo_type=repo_type, revision=revision)\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 2844, in repo_info\n",
      "    return method(\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 2628, in model_info\n",
      "    r = get_session().get(path, headers=headers, timeout=timeout, params=params)\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/requests/sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 96, in send\n",
      "    return super().send(request, *args, **kwargs)\n",
      "  File \"/Users/hua/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/requests/adapters.py\", line 698, in send\n",
      "    raise SSLError(e, request=request)\n",
      "requests.exceptions.SSLError: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/OpenVINO/FLUX.1-schnell-int4-ov/revision/main (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1017)')))\"), '(Request ID: 6e0319db-8f34-4ded-aa15-65d9c44aceeb)')\n"
     ]
    }
   ],
   "source": [
    "from cmd_helper import optimum_cli\n",
    "\n",
    "if not model_dir.exists():\n",
    "    if not use_preconverted.value:\n",
    "        optimum_cli(model_id, model_dir, additional_args=additional_args)\n",
    "    else:\n",
    "        ov_model_id = f\"OpenVINO/{model_id.split('/')[-1]}-{model_dir.name.lower()}-ov\"\n",
    "        !huggingface-cli download {ov_model_id} --local-dir {model_dir}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bf6b6a4-0fc0-425d-acbf-5ac0df8dd993",
   "metadata": {},
   "source": [
    "## Run OpenVINO GenAI model inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Select device from dropdown list for running inference using OpenVINO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "719aa55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e240eda8710a4ba6a3abc28accf17b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'AUTO'), value='CPU')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from notebook_utils import device_widget\n",
    "\n",
    "device = device_widget(default=\"CPU\", exclude=[\"NPU\"])\n",
    "device\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1d71c44-761a-45f8-858c-e60a9a625dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888f16f8ae094f9983dd0b41b6c16d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Use compressed models', disabled=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "model_available = (model_base_dir / \"INT4\").is_dir()\n",
    "use_quantized_models = widgets.Checkbox(\n",
    "    value=model_available,\n",
    "    description=\"Use compressed models\",\n",
    "    disabled=not model_available,\n",
    ")\n",
    "\n",
    "use_quantized_models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cab6790e",
   "metadata": {},
   "source": [
    "`openvino_genai.Text2ImagePipeline` represents inference pipeline for text-to-image generation. For creation pipeline instance, you should provide directory with converted to OpenVINO model and inference device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7230b71",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenvino_genai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mov_genai\u001b[39;00m\n\u001b[1;32m      3\u001b[0m model_dir \u001b[38;5;241m=\u001b[39m model_base_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINT4\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_quantized_models\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01melse\u001b[39;00m model_base_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m ov_pipe \u001b[38;5;241m=\u001b[39m ov_genai\u001b[38;5;241m.\u001b[39mText2ImagePipeline(model_dir, device\u001b[38;5;241m=\u001b[39m\u001b[43mdevice\u001b[49m\u001b[38;5;241m.\u001b[39mvalue)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "import openvino_genai as ov_genai\n",
    "\n",
    "model_dir = model_base_dir / \"INT4\" if use_quantized_models.value else model_base_dir / \"FP16\"\n",
    "\n",
    "ov_pipe = ov_genai.Text2ImagePipeline(model_dir, device=device.value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75aeb184",
   "metadata": {},
   "source": [
    "Now, you can define a text prompt and other pipeline settings for image generation and run inference pipeline.\n",
    "\n",
    "> **Note**: Consider increasing `num_inference_steps` to get more precise results, but higher value will take longer time to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49dc632b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline settings\n",
      "Input text: A cat holding a sign that says hello OpenVINO\n",
      "Image size: 256 x 256\n",
      "Seed: 42\n",
      "Number of steps: 4\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A cat holding a sign that says hello OpenVINO\"\n",
    "height = 256\n",
    "width = 256\n",
    "seed = 42\n",
    "num_inference_steps = 4\n",
    "\n",
    "print(\"Pipeline settings\")\n",
    "print(f\"Input text: {prompt}\")\n",
    "print(f\"Image size: {height} x {width}\")\n",
    "print(f\"Seed: {seed}\")\n",
    "print(f\"Number of steps: {num_inference_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc06a9c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af13999018344a09e72bec8fd2c6a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/stilex/openvino-genai-scripts/metadata.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m latencies \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     22\u001b[0m num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[0;32m---> 23\u001b[0m metadata_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/stilex/openvino-genai-scripts/metadata.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m selected_requests \u001b[38;5;241m=\u001b[39m metadata_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m:num_examples]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m selected_requests\u001b[38;5;241m.\u001b[39miterrows():\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/pandas/io/parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    275\u001b[0m         path_or_handle,\n\u001b[1;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/pandas/io/parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/stilex/openvino-genai-scripts/metadata.parquet'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "random_generator = ov_genai.TorchGenerator(seed)\n",
    "pbar = tqdm(total=num_inference_steps)\n",
    "\n",
    "def callback(step, num_steps, latent):\n",
    "    if num_steps != pbar.total:\n",
    "        pbar.reset(num_steps)\n",
    "    pbar.update(1)\n",
    "    sys.stdout.flush()\n",
    "    return False\n",
    "\n",
    "images_directory = './flux_generated_images'\n",
    "os.makedirs(images_directory, exist_ok=True)\n",
    "\n",
    "latencies = []\n",
    "num_examples = 200\n",
    "metadata_df = pd.read_parquet('/home/stilex/openvino-genai-scripts/metadata.parquet')\n",
    "selected_requests = metadata_df.iloc[0:num_examples].copy()\n",
    "for i, row in selected_requests.iterrows():\n",
    "    prompt = row['prompt']\n",
    "    clean_prompt = re.sub(r'[^\\w\\-_\\.]', '_', prompt)[:230]\n",
    "    image_path = f\"{images_directory}/{clean_prompt}.png\"\n",
    "    start_time = time.time()\n",
    "    result = ov_pipe.generate(prompt, num_inference_steps=num_inference_steps, generator=random_generator, callback=callback, height=height, width=width)\n",
    "    end_time = time.time()\n",
    "    latency = end_time - start_time\n",
    "    latencies.append(latency)\n",
    "    final_image = Image.fromarray(result.data[0])\n",
    "    final_image.save(image_path)\n",
    "    final_image\n",
    "\n",
    "\n",
    "print(f\"🕒 Avg Latency: {sum(latencies)/len(latencies)} seconds\")\n",
    "pbar.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7b8cee1-9e46-46f6-a4ef-b3bc132aaa01",
   "metadata": {},
   "source": [
    "## Interactive demo\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88519e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gradio_helper import make_demo\n",
    "\n",
    "demo = make_demo(ov_pipe, model_name=str(model_base_dir))\n",
    "\n",
    "# if you are launching remotely, specify server_name and server_port\n",
    "#  demo.launch(server_name='your server name', server_port='server port in int')\n",
    "# if you have any issue to launch on your platform, you can pass share=True to launch method:\n",
    "# demo.launch(share=True)\n",
    "# it creates a publicly shareable link for the interface. Read more in the docs: https://gradio.app/docs/\n",
    "try:\n",
    "    demo.launch(debug=True)\n",
    "except Exception:\n",
    "    demo.launch(debug=True, share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-gpu)",
   "language": "python",
   "name": "torch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/user-attachments/assets/11733314-0b31-449c-9885-12ebf6365a58",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [
     "Stable Diffusion"
    ],
    "tasks": [
     "Text-to-Image"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
